{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sort-Based-Index](img/treclogo-c.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conferences\n",
    "\n",
    "Special Interest Group on Information Retrieval: <a href='https://sigir.org'>SIGIR</a> <br>\n",
    "Text REtrieval Conference: <a href='https://trec.nist.gov/proceedings/proceedings.html'>TREC</a>  <br>\n",
    "European Conference on Information Retrieval: <a href='http://www.ecir2018.org/'>ECIR</a>  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example papers:\n",
    "<ul>\n",
    "<li><a href='http://nrl.northumbria.ac.uk/30863/1/SIGIR2017_Elsweiler.pdf'>Exploiting Food Choice Biases for Healthier <b>Recipe Recommendation</b></a> -> dataset of food reciped with nutrition information crawled from Allrecipes.com\n",
    "<li>CitySearcher: A <b>City Search</b> Engine For Interests \n",
    "<li>A Test Collection for Evaluating <b>Legal Case Law Search</b>\n",
    "<li>Multihop Attention Networks for <b>Question Answer Matching</b>\n",
    "<li>Semantic Location in <b>Email Query Suggestion</b>\n",
    "<li>Online <b>Job Search</b>: Study of Usersâ€™ Search Behavior using Search Engine Query Logs\t\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Valued Projects:\n",
    "\n",
    "- Something Useful for Sofia University, the Master's Degree, etc. (contact Prof. Koychev)\n",
    "- Participating in Shared Tasks (contact us or Prof. Koychev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Shared Tasks\n",
    "\n",
    "## <a href='http://alt.qcri.org/semeval2019/index.php?id=tasks'>SemEval</a>\n",
    "- Fact Checking in Community Question Answering Forums\n",
    "- Suggestion Mining from Online Reviews and Forums\n",
    "- RumourEval 2019: Determining Rumour Veracity and Support for Rumours\n",
    "- many more\n",
    "\n",
    "tbc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incidence Matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bbc_news_sentences = [\n",
    "    \"China confirms Interpol chief detained\",\n",
    "    \"Turkish officials believe the Washington Post writer was killed in the Saudi consulate in Istanbul.\",\n",
    "    \"US wedding limousine crash kills 20\",\n",
    "    \"Bulgarian journalist killed in park\",\n",
    "    \"Kanye West deletes social media profiles\",\n",
    "    \"Brazilians vote in polarised election\",\n",
    "    \"Bull kills woman at French festival\",\n",
    "    \"Indonesia to wrap up tsunami search\",\n",
    "    \"Tina Turner reveals wedding night ordeal\",\n",
    "    \"Victory for Trump in Supreme Court battle\",\n",
    "    \"Clashes at German far-right rock concert\",\n",
    "    \"The Walking Dead actor dies aged 76\",\n",
    "    \"Jogger in Netherlands finds lion cub\",\n",
    "    \"Monkey takes the wheel of Indian bus\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['China', 'confirms', 'Interpol', 'chief', 'detained']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic tokenization\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "sample_bbc_news_sentences_tokenized = [tokenizer.tokenize(sent) for sent in sample_bbc_news_sentences]\n",
    "sample_bbc_news_sentences_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china', 'confirms', 'interpol', 'chief', 'detained']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bbc_news_sentences_tokenized_lower = [[_t.lower() for _t in _s] for _s in sample_bbc_news_sentences_tokenized]\n",
    "sample_bbc_news_sentences_tokenized_lower[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '20',\n",
       " '76',\n",
       " 'actor',\n",
       " 'aged',\n",
       " 'at',\n",
       " 'battle',\n",
       " 'believe',\n",
       " 'brazilians',\n",
       " 'bulgarian',\n",
       " 'bull',\n",
       " 'bus',\n",
       " 'chief',\n",
       " 'china',\n",
       " 'clashes',\n",
       " 'concert',\n",
       " 'confirms',\n",
       " 'consulate',\n",
       " 'court',\n",
       " 'crash',\n",
       " 'cub',\n",
       " 'dead',\n",
       " 'deletes',\n",
       " 'detained',\n",
       " 'dies',\n",
       " 'election',\n",
       " 'far-right',\n",
       " 'festival',\n",
       " 'finds',\n",
       " 'for',\n",
       " 'french',\n",
       " 'german',\n",
       " 'in',\n",
       " 'indian',\n",
       " 'indonesia',\n",
       " 'interpol',\n",
       " 'istanbul',\n",
       " 'jogger',\n",
       " 'journalist',\n",
       " 'kanye',\n",
       " 'killed',\n",
       " 'kills',\n",
       " 'limousine',\n",
       " 'lion',\n",
       " 'media',\n",
       " 'monkey',\n",
       " 'netherlands',\n",
       " 'night',\n",
       " 'of',\n",
       " 'officials',\n",
       " 'ordeal',\n",
       " 'park',\n",
       " 'polarised',\n",
       " 'post',\n",
       " 'profiles',\n",
       " 'reveals',\n",
       " 'rock',\n",
       " 'saudi',\n",
       " 'search',\n",
       " 'social',\n",
       " 'supreme',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'tina',\n",
       " 'to',\n",
       " 'trump',\n",
       " 'tsunami',\n",
       " 'turkish',\n",
       " 'turner',\n",
       " 'up',\n",
       " 'us',\n",
       " 'victory',\n",
       " 'vote',\n",
       " 'walking',\n",
       " 'was',\n",
       " 'washington',\n",
       " 'wedding',\n",
       " 'west',\n",
       " 'wheel',\n",
       " 'woman',\n",
       " 'wrap',\n",
       " 'writer'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all unique tokens\n",
    "unique_tokens = set(sum(sample_bbc_news_sentences_tokenized_lower, []))\n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# create incidence matrix (term-document frequency)\n",
    "import numpy as np\n",
    "incidence_matrix = np.array([[sent.count(token) for sent in sample_bbc_news_sentences_tokenized_lower] \n",
    "                    for token in unique_tokens])\n",
    "print(incidence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a bigger vocab can take too much memory (number of tokens * number of documents), while also being sparse!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inverted Index](img/inverted-index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words will have highest and which lowest Total freq?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52464 52830 53589 53676 53750 53820 53871 53918 54041 54111 54157 54248 54337\n",
      "52758 53508 53640 53683 53769 53824 53872 53921 54042 54114 54160 54255 54353\n",
      "52766 53511 53641 53692 53772 53829 53891 53935 54057 54115 54165 54265 54489\n",
      "52792 53521 53653 53706 53777 53837 53892 53938 54066 54122 54175 54302 54490\n",
      "52794 53529 53655 53708 53804 53839 53909 53971 54069 54132 54176 54305\n",
      "52817 53569 53664 53712 53808 53850 53911 53976 54090 54140 54212 54306\n",
      "52820 53574 53669 53741 53812 53865 53913 53986 54092 54143 54224 54310\n",
      "52822 53584 53675 53742 53818 53868 53915 54010 54096 54147 54244 54325\n",
      "Lines: 48\n",
      "\n",
      "In article <1993Mar25.161909.8110@wuecl.wustl.edu> dp@cec1.wustl.edu (David Prutchi) writes:\n",
      ">In article <C4CntG.Jv4@spk.hp.com> long@spk.hp.com (Jerry Long) writes:\n",
      ">>Fred W. Culpepper (fculpepp@norfolk.vak12ed.edu) wrote:\n",
      ">>[...]\n",
      ">>A couple of years ago I put together a Tesla circuit which\n",
      ">>was published in an electronics magazine and could have been\n",
      ">>the circuit which is referred to here. This one used a\n",
      ">>flyback transformer from a tv onto which you wound your own\n"
     ]
    }
   ],
   "source": [
    "!ls data/mini_newsgroups/sci.electronics/\n",
    "!tail -50 data/mini_newsgroups/sci.electronics/52464 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You will now have to construct the Inverted Index - only the dictionary part (term and #docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "    \n",
    "def prepare_dataset(documents_dir):\n",
    "    tokenized_documents = []\n",
    "    for document in os.listdir(documents_dir):\n",
    "        with open(os.path.join(documents_dir, document)) as outf:\n",
    "            sentence_tokens = [tokenizer.tokenize(sent.lower()) for sent in sent_tokenize(outf.read())]\n",
    "            tokenized_documents.append(sum(sentence_tokens, []))\n",
    "    print(\"Found documents: \", len(tokenized_documents))\n",
    "    return tokenized_documents      \n",
    "    \n",
    "def document_frequency(tokenized_documents):\n",
    "    all_unique_tokens = set(sum(tokenized_documents, []))\n",
    "    print(\"Found unique tokens: \", len(all_unique_tokens))\n",
    "    \n",
    "    tokens = {token: sum([1 for doc in tokenized_documents if token in doc]) \n",
    "                    for token in all_unique_tokens}\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/mini_newsgroups/sci.crypt/\n",
      "Found documents:  100\n",
      "Sample tokenized document:\n",
      "['newsgroups', ':', 'sci.crypt', 'path', ':', 'cantaloupe.srv.cs.cmu.edu', '!', 'rochester', '!', 'udel']\n",
      "Found unique tokens:  5801\n",
      "Most common words:\n",
      "[('from', 100), ('apr', 100), ('newsgroups', 100), ('.', 100), ('path', 100), ('sci.crypt', 100), ('date', 100), ('message-id', 100), ('!', 100), ('subject', 100)]\n",
      "Least common words:\n",
      "[('tied', 1), ('quality', 1), ('plug', 1), ('<1993apr21.215131.29323@slcs.slb.com>', 1), ('hole', 1), ('camelot', 1), ('publically', 1), ('pundits', 1), ('society', 1), (\"deconcini's\", 1)]\n"
     ]
    }
   ],
   "source": [
    "selected_category = 'data/mini_newsgroups/sci.crypt/'\n",
    "print(selected_category)\n",
    "tokenized_dataset = prepare_dataset(selected_category)\n",
    "print(\"Sample tokenized document:\")\n",
    "print(tokenized_dataset[0][:10])\n",
    "\n",
    "df = document_frequency(tokenized_dataset)\n",
    "\n",
    "print(\"Most common words:\")\n",
    "print(Counter(df).most_common(10))\n",
    "print(\"Least common words:\")\n",
    "print(Counter(df).most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sort-Based-Index](img/sort-based-method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next in research:\n",
    "\n",
    "- <a href='http://nbjl.nankai.edu.cn/Lab_Papers/2018/SIGIR2018.pdf'>Index Compression for BitFunnel Query Processing</a>\n",
    "- <a href='https://link.springer.com/chapter/10.1007/978-3-319-76941-7_47'>Inverted List Caching for Topical Index Shards</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
